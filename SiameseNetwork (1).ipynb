{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, GlobalAveragePooling2D, MaxPool2D, Dropout, BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model  #the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Making required directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the directory structure for our project. Namely, three folders Anchor, Negative, Positive\n",
    "pos_path = os.path.join('data', 'positive')         #positive images (verification images)\n",
    "neg_path = os.path.join('data', 'negative')         #negative images (different from the anchor images)\n",
    "anc_path = os.path.join('data', 'anchor')           #input image (object to be recognized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pos_path)\n",
    "os.makedirs(neg_path)\n",
    "os.makedirs(anc_path)\n",
    "#first, we collect the negative examples through Labelled Faces in the wild repository\n",
    "! tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(neg_path, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)\n",
    "\n",
    "\"\"\"\n",
    "first we loop over every sub directory in the lfw folder. Then for every file in each sub directory, we get the path of the file, create a new path for the\n",
    "file and the replace the two paths (in effect cutting the files from the initial location to the final location)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Writing a function to capture User's image through the webcam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capturing the anchor and the positive images using our webcam and opencv\n",
    "#step1. establish a connection to the webcam using VideoCapture()\n",
    "cap = cv2.VideoCapture(-1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow(\"Image Collection\", frame)\n",
    "    if(cv2.waitKey(1) & 0XFF == ord('q')):\n",
    "        break\n",
    "#Release the webcam and destroy the image show frame\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "#this is actually the last frame captured by the camera before i pressed q (as soon as it is pressed, the image capture stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However, the resolution is large and needs to be reduced for processing\n",
    "#for this we make a change to the original image capturing function\n",
    "#capturing the anchor and the positive images using our webcam and opencv\n",
    "#step1. establish a connection to the webcam using VideoCapture()\n",
    "cap = cv2.VideoCapture(-1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:370, 200:450, :]  #arbitrary values\n",
    "    #collecting anchor images\n",
    "    if(cv2.waitKey(1) & 0XFF == ord('a')):\n",
    "        imgname = os.path.join(anc_path, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "\n",
    "    #collecting positive images\n",
    "    if(cv2.waitKey(1) & 0XFF == ord('p')):\n",
    "        imgname = os.path.join(pos_path, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "\n",
    "    cv2.imshow(\"Image Collection\", frame)\n",
    "    if(cv2.waitKey(1) & 0XFF == ord('q')):\n",
    "        break\n",
    "#Release the webcam and destroy the image show frame\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "#last image captured by the camera. Also, in this case, the resolution is smaller than the pervious case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(5):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(pos_path)):\n",
    "    img_path = os.path.join(pos_path, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    augmented_images = data_aug(img) \n",
    "    \n",
    "    for image in augmented_images:\n",
    "        cv2.imwrite(os.path.join(pos_path, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(anc_path)):\n",
    "    img_path = os.path.join(anc_path, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    augmented_images = data_aug(img) \n",
    "    \n",
    "    for image in augmented_images:\n",
    "        cv2.imwrite(os.path.join(anc_path, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Preprocessing for the Neural Network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting the image directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(anc_path +\"/*.jpg\").take(4000)\n",
    "positive = tf.data.Dataset.list_files(pos_path +\"/*.jpg\").take(4000)\n",
    "negative = tf.data.Dataset.list_files(neg_path +\"/*.jpg\").take(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create a performant data pipeline, we first create a function which is mapped to the entire dataset\n",
    "def preprocess(file_path):\n",
    "    #reading in image from filepath\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    img = img/255.0   #rescaling for better learning\n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (anchor, positive) = 1,1,1,1,1\n",
    "# (anchor, negative) = 0,0,0,0,0\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing what is happening\n",
    "sample = data.as_numpy_iterator()\n",
    "sample.next()\n",
    "#here, it has paired an anchor and a positive image and labelled the pair to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)\n",
    "\n",
    "res = preprocess_twin(*sample.next())\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(res[0])\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(res[1])\n",
    "plt.axis(False)\n",
    "\n",
    "#this is an example of anchor and positive image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the data loader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()\n",
    "res = samples.next()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(res[0])\n",
    "plt.axis(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(res[1])\n",
    "plt.axis(False)\n",
    "\n",
    "#this is an example of anchor and negative image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the entire data into training and testing data\n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = data.skip(round(len(data)*0.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape = (100,100,3))\n",
    "    #first block\n",
    "    x = Conv2D(filters = 64, kernel_size = (10,10), strides = 1, activation = \"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(64, (2,2), padding = \"same\")(x)\n",
    "\n",
    "    #second block\n",
    "    x = Conv2D(filters = 128, kernel_size = (7,7), strides = 1, activation = \"relu\")(x)\n",
    "    x = MaxPool2D(64, (2,2), padding = \"same\")(x)\n",
    "\n",
    "    #third block\n",
    "    x = Conv2D(filters = 128, kernel_size = (4,4), strides = 1, activation = \"relu\")(x)\n",
    "    x = MaxPool2D(64, (2,2), padding = \"same\")(x)\n",
    "\n",
    "    #fourth block\n",
    "    x = Conv2D(filters = 256, kernel_size = (4,4), strides = 1, activation = \"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(4096, activation = \"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs = [inp], outputs = [outputs] , name = \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_embedding()\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the distance layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Siamese L1 distance class\n",
    "class L1Dist(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making the Siamese Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()\n",
    "def make_siamese_model():\n",
    "\n",
    "    #Handle inputs\n",
    "    input_image = Input(name = \"input_image\", shape = (100,100,3))                  #anchor image\n",
    "    validation_image = Input(name = \"validation_image\", shape = (100,100,3))         #negative or positive\n",
    "\n",
    "    #combine Siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    #classification layer\n",
    "    classifier = Dense(1, activation = \"sigmoid\")(distances)\n",
    "    return Model(inputs = [input_image, validation_image], outputs = classifier, name = \"SiameseNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "plot_model(siamese_model, show_shapes = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting up Training Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = train_data.as_numpy_iterator().next()\n",
    "print(np.array(test_sample[:2]).shape)\n",
    "#2 corresponds to both validation and input image (say anchor and positive or anchor and negative)\n",
    "#16 refers to the batch size\n",
    "#100, 100, 3 refers to the size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "#establish checkpoints\n",
    "checkpoint_dir = \"./training checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"chkpt\")\n",
    "checkpoint = tf.train.Checkpoint(opt = optimizer, siamese_model = siamese_model)\n",
    "\n",
    "\n",
    "\n",
    "#building the train step function (what happens when a mini batch is completely trained)\n",
    "@tf.function   #compiles a function into a callable tensorflow graph\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "\n",
    "        yhat = siamese_model(X, training = True)\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    #calculate the gradients. Gradient Tape records the order in which operations happened during ForwardProp and uses that to find gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, siamese_model.trainable_variables))           #updates to the weights\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Creating a metric object \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            \n",
    "        progbar.update(idx+1)\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "print(test_input.shape)\n",
    "yhat = siamese_model.predict([test_input, test_val])\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing the results of the model\n",
    "[1 if prediction > 0.5  else 0 for prediction in yhat]  #list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)   #as we can see, the predictions and the model actually line up pretty well for this example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('siamesemodel.h5')\n",
    "model = tf.keras.models.load_model('siamesemodel.h5', custom_objects = {'L1Dist': L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = siamese_model.predict([test_input, test_val])\n",
    "prediction = []\n",
    "true = []\n",
    "for input_image, validation_image, y_true in test_data:\n",
    "    prediction.append(tf.squeeze(siamese_model.predict([input_image, validation_image])))\n",
    "    true.append(tf.squeeze(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(true))\n",
    "print(len(prediction))\n",
    "true_final = []\n",
    "for x in true:\n",
    "    for elem in x:\n",
    "        true_final.append(int(elem.numpy()))\n",
    "print(true_final[:10])\n",
    "prediction_final = []\n",
    "for y in prediction:\n",
    "    for elem in y:\n",
    "        prediction_final.append(elem.numpy())\n",
    "prediction_final = [1 if prediction > 0.5  else 0 for prediction in prediction_final]\n",
    "print(prediction_final[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_final == prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true = true_final, y_pred =prediction_final, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_final == prediction_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Real Time test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat) \n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join('application_data', 'verification_images'))\n",
    "os.path.join('application_data', 'input_image', 'input_image.jpg')\n",
    "for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "    validation_img = os.path.join('application_data', 'verification_images', image)\n",
    "    print(validation_img)\n",
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        \n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images'))) \n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(-1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save input image to application_data/input_image folder \n",
    "#         hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "#         h, s, v = cv2.split(hsv)\n",
    "\n",
    "#         lim = 255 - 10\n",
    "#         v[v > lim] = 255\n",
    "#         v[v <= lim] -= 10\n",
    "        \n",
    "#         final_hsv = cv2.merge((h, s, v))\n",
    "#         img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        # Run verification\n",
    "        results, verified = verify(siamese_model, 0.5, 0.5)\n",
    "        print(verified)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
